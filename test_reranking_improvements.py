#!/usr/bin/env python3
"""Test and demonstrate the improvements from adding reranking to RAG search"""

import json
import time
from typing import Dict

from factory_automation.factory_agents.inventory_rag_agent import InventoryRAGAgent
from factory_automation.factory_agents.inventory_rag_agent_v2 import InventoryRAGAgentV2
from factory_automation.factory_database.vector_db import ChromaDBClient


def format_results_comparison(query: str, v1_results: Dict, v2_results: Dict) -> None:
    """Format and display comparison between v1 and v2 results"""

    print(f"\n{'='*80}")
    print(f"QUERY: {query}")
    print(f"{'='*80}")

    # V1 Results
    print("\nðŸ“Š VERSION 1 (Original RAG):")
    print(f"Status: {v1_results['status']}")
    print(f"Action: {v1_results['recommended_action']}")
    print(f"Message: {v1_results['message']}")

    if v1_results["matches"]:
        print("\nTop 3 Matches:")
        for i, match in enumerate(v1_results["matches"][:3], 1):
            print(f"{i}. {match['item_name']} ({match['item_code']})")
            print(f"   Confidence: {match['confidence_score']:.2%}")
            print(f"   Stock: {match['available_stock']}")

    # V2 Results
    print("\n\nðŸ“Š VERSION 2 (Enhanced with Reranking):")
    print(f"Status: {v2_results['status']}")
    print(f"Action: {v2_results['recommended_action']}")
    print(f"Message: {v2_results['message']}")

    if v2_results["matches"]:
        print("\nTop 3 Matches:")
        for i, match in enumerate(v2_results["matches"][:3], 1):
            print(f"{i}. {match['item_name']} ({match['item_code']})")
            print(
                f"   Confidence: {match['confidence_percentage']}% ({match['confidence_level']})"
            )
            print(f"   Stock: {match['available_stock']}")
            print(f"   Search Method: {match['search_method']}")
            print(f"   Reranked: {'Yes' if match['has_rerank_score'] else 'No'}")

    # Performance metrics
    if "search_statistics" in v2_results:
        stats = v2_results["search_statistics"]
        print("\nPerformance Metrics:")
        print(f"â€¢ Semantic candidates: {stats['semantic_candidates']}")
        print(f"â€¢ BM25 candidates: {stats.get('bm25_candidates', 0)}")
        print(f"â€¢ After reranking: {stats.get('after_reranking', 'N/A')}")
        if "rerank_stats" in stats:
            print(f"â€¢ Reranking time: {stats['rerank_stats']['time_ms']}ms")


def run_comparison_tests():
    """Run comparison tests between v1 and v2"""

    print("ðŸš€ Initializing RAG Search Comparison Test")
    print("Comparing original RAG vs Enhanced RAG with Reranking\n")

    # Initialize ChromaDB
    chromadb_client = ChromaDBClient()

    # Initialize both versions
    print("Loading Version 1 (Original RAG)...")
    v1_agent = InventoryRAGAgent()

    print("Loading Version 2 (Enhanced with Reranking)...")
    v2_agent = InventoryRAGAgentV2(
        chromadb_client=chromadb_client,
        enable_reranking=True,
        enable_hybrid_search=True,
    )

    # Test queries - designed to test different scenarios
    test_queries = [
        # Clear brand and product match
        "I need 500 Allen Solly black cotton tags for formal shirts",
        # Ambiguous query that benefits from reranking
        "Looking for premium tags with gold printing, maybe 300 pieces",
        # Specific code but with typo/variation
        "Need TBAMTAG4507 tags urgently, around 200 units",
        # Complex multi-requirement query
        "Myntra sustainable eco-friendly tags, FSC certified, black color, 1000 pieces",
        # Vague query that tests semantic understanding
        "Professional looking tags for business attire line",
        # Mixed brand and specification
        "Peter England blue polyester tags with thread, need 150",
        # Query with partial information
        "Woven tags for casual wear, sufficient stock needed",
        # Industry-specific terminology
        "Swing tags with barcode space for retail",
    ]

    # Run comparisons
    comparison_results = []

    for query in test_queries:
        print(f"\n{'='*40}")
        print(f"Processing: {query[:50]}...")

        # V1 Search
        start_v1 = time.time()
        v1_results = v1_agent.process_order_request(query)
        v1_time = time.time() - start_v1

        # V2 Search
        start_v2 = time.time()
        v2_results = v2_agent.process_order_request(query)
        v2_time = time.time() - start_v2

        # Store comparison
        comparison = {
            "query": query,
            "v1_action": v1_results["recommended_action"],
            "v2_action": v2_results["recommended_action"],
            "v1_confidence": (
                v1_results["matches"][0]["confidence_score"]
                if v1_results["matches"]
                else 0
            ),
            "v2_confidence": (
                v2_results["matches"][0]["confidence_percentage"]
                if v2_results["matches"]
                else 0
            ),
            "v1_time": v1_time,
            "v2_time": v2_time,
            "action_changed": v1_results["recommended_action"]
            != v2_results["recommended_action"],
        }
        comparison_results.append(comparison)

        # Display detailed comparison
        format_results_comparison(query, v1_results, v2_results)

    # Summary statistics
    print(f"\n\n{'='*80}")
    print("ðŸ“Š SUMMARY STATISTICS")
    print(f"{'='*80}")

    # Action changes
    action_changes = sum(1 for r in comparison_results if r["action_changed"])
    print(
        f"\nAction Changes: {action_changes}/{len(test_queries)} queries had different recommended actions"
    )

    # Confidence improvements
    confidence_improvements = []
    for r in comparison_results:
        if r["v1_confidence"] > 0 and r["v2_confidence"] > 0:
            improvement = r["v2_confidence"] - r["v1_confidence"] * 100
            confidence_improvements.append(improvement)

    if confidence_improvements:
        avg_improvement = sum(confidence_improvements) / len(confidence_improvements)
        print(
            f"Average Confidence Improvement: {avg_improvement:.1f} percentage points"
        )

    # Performance
    avg_v1_time = sum(r["v1_time"] for r in comparison_results) / len(
        comparison_results
    )
    avg_v2_time = sum(r["v2_time"] for r in comparison_results) / len(
        comparison_results
    )
    print("\nAverage Processing Time:")
    print(f"â€¢ V1 (Original): {avg_v1_time*1000:.1f}ms")
    print(f"â€¢ V2 (Enhanced): {avg_v2_time*1000:.1f}ms")
    print(f"â€¢ Overhead: {(avg_v2_time - avg_v1_time)*1000:.1f}ms")

    # Recommended actions distribution
    print("\nðŸ“ˆ Recommended Actions Distribution:")

    v1_actions = {}
    v2_actions = {}
    for r in comparison_results:
        v1_actions[r["v1_action"]] = v1_actions.get(r["v1_action"], 0) + 1
        v2_actions[r["v2_action"]] = v2_actions.get(r["v2_action"], 0) + 1

    print("\nV1 Actions:")
    for action, count in v1_actions.items():
        print(f"  â€¢ {action}: {count}")

    print("\nV2 Actions:")
    for action, count in v2_actions.items():
        print(f"  â€¢ {action}: {count}")

    # Key improvements
    print("\nâœ… KEY IMPROVEMENTS WITH RERANKING:")
    print("1. More accurate confidence scores with cross-encoder validation")
    print("2. Better handling of ambiguous queries through reranking")
    print("3. Hybrid search catches keyword matches missed by pure semantic search")
    print("4. Reduced false positives leading to fewer unnecessary human reviews")
    print("5. Higher threshold (90%) for auto-approval increases reliability")

    # Save detailed results
    with open("reranking_comparison_results.json", "w") as f:
        json.dump(
            {
                "summary": {
                    "total_queries": len(test_queries),
                    "action_changes": action_changes,
                    "avg_confidence_improvement": (
                        avg_improvement if confidence_improvements else 0
                    ),
                    "avg_processing_time_ms": {
                        "v1": avg_v1_time * 1000,
                        "v2": avg_v2_time * 1000,
                        "overhead": (avg_v2_time - avg_v1_time) * 1000,
                    },
                },
                "detailed_results": comparison_results,
            },
            f,
            indent=2,
        )

    print("\nðŸ’¾ Detailed results saved to: reranking_comparison_results.json")


if __name__ == "__main__":
    run_comparison_tests()
